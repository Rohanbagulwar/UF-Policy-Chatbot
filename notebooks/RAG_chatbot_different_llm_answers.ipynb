{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48a089d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "import os\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e34a0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chromadb(persist_directory=\"./chroma_db\", collection_name=\"policies\"):\n",
    "    \"\"\"\n",
    "    Initialize ChromaDB client and create/get collection.\n",
    "    \n",
    "    Args:\n",
    "        persist_directory: Local directory to persist the database\n",
    "        collection_name: Name of the collection\n",
    "    \n",
    "    Returns:\n",
    "        ChromaDB collection object\n",
    "    \"\"\"\n",
    "    # Create persist directory if it doesn't exist\n",
    "    os.makedirs(persist_directory, exist_ok=True)\n",
    "    \n",
    "    # Initialize ChromaDB client with persistence\n",
    "    client = chromadb.PersistentClient(path=persist_directory)\n",
    "    \n",
    "    print(f\"‚úì ChromaDB initialized at: {persist_directory}\")\n",
    "    \n",
    "    # Get or create collection\n",
    "    try:\n",
    "        # Try to get existing collection\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "        print(f\"‚úì Retrieved existing collection: {collection_name}\")\n",
    "        print(f\"  Current documents: {collection.count()}\")\n",
    "    except:\n",
    "        # Create new collection if doesn't exist\n",
    "        collection = client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": \"UF Policy documents with embeddings\"}\n",
    "        )\n",
    "        print(f\"‚úì Created new collection: {collection_name}\")\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e71add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(model_name=\"BAAI/bge-small-en-v1.5\"):\n",
    "    \"\"\"\n",
    "    Load BGE embedding model from HuggingFace.\n",
    "    \n",
    "    Available BGE models:\n",
    "    - BAAI/bge-base-en-v1.5 (balanced, 768 dimensions)\n",
    "    Args:\n",
    "        model_name: HuggingFace model identifier\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer model\n",
    "    \"\"\"\n",
    "    print(f\"Loading embedding model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(f\"‚úì Model loaded successfully!\")\n",
    "    print(f\"  Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ce31143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb(collection, model, query_text, n_results=5, filter_by_type=None):\n",
    "    \"\"\"\n",
    "    Query ChromaDB with a text query.\n",
    "    \n",
    "    Args:\n",
    "        collection: ChromaDB collection\n",
    "        model: SentenceTransformer model for query embedding\n",
    "        query_text: Query string\n",
    "        n_results: Number of results to return\n",
    "        filter_by_type: Filter results by policy type (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Query results\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = model.encode([query_text])[0].tolist()\n",
    "    \n",
    "    # Prepare filter\n",
    "    where_filter = None\n",
    "    if filter_by_type:\n",
    "        where_filter = {\"type\": {\"$eq\": filter_by_type}}\n",
    "    \n",
    "    # Query ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        where=where_filter\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(\n",
    "                  persist_directory=\"./chroma_db\",\n",
    "                  collection_name=\"policies\",\n",
    "                  model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "                  include_types=None,\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    Main pipeline to load data, create embeddings, and store in ChromaDB.\n",
    "    \n",
    "    Args:\n",
    "        excel_file: Path to Excel file\n",
    "        persist_directory: ChromaDB storage directory\n",
    "        collection_name: Name of ChromaDB collection\n",
    "        model_name: BGE model name\n",
    "        include_types: List of types to include (e.g., ['Policy', 'Regulation'])\n",
    "        overwrite: If True, delete existing collection and create new one\n",
    "    \n",
    "    Returns:\n",
    "        collection: ChromaDB collection object\n",
    "        model: Embedding model\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Step 2: Load embedding model\n",
    "    print(f\"\\nStep 2: Loading embedding model...\")\n",
    "    embedding_model = load_embedding_model(model_name)\n",
    "    \n",
    "    # Step 3: Initialize ChromaDB\n",
    "    print(f\"\\nStep 3: Initializing ChromaDB...\")\n",
    "    \n",
    "    collection = initialize_chromadb(persist_directory, collection_name)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úì PIPELINE COMPLETE!\")\n",
    "    print(f\"‚úì Total documents in collection: {collection.count()}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection, model = main_pipeline(\n",
    "                persist_directory=\"./chroma_db\",\n",
    "                collection_name=\"policies\",\n",
    "    model_name=\"BAAI/bge-base-en-v1.5\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eac0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_openai_client(api_key: str, base_url: str = 'https://api.ai.it.ufl.edu'):\n",
    "    \"\"\"\n",
    "    Initialize OpenAI client with custom base URL.\n",
    "    \n",
    "    Args:\n",
    "        api_key: Your OpenAI API key\n",
    "        base_url: Custom base URL for API (default: UF API endpoint)\n",
    "    \n",
    "    Returns:\n",
    "        OpenAI client object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=base_url\n",
    "        )\n",
    "        \n",
    "        print('‚úì OpenAI client created successfully')\n",
    "        print(f'  Base URL: {base_url}')\n",
    "        print(f'  Client type: {type(client)}')\n",
    "        \n",
    "        # Sanity checks\n",
    "        has_chat = hasattr(client, 'chat')\n",
    "        print(f'  Has chat attribute: {has_chat}')\n",
    "        \n",
    "        if has_chat:\n",
    "            has_completions = hasattr(client.chat, 'completions')\n",
    "            print(f'  Has completions attribute: {has_completions}')\n",
    "        \n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f' Error initializing OpenAI client: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57987876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "def prepare_context_from_results(query_results: Dict[str, Any]) -> tuple:\n",
    "    \"\"\"\n",
    "    Prepare context and metadata from ChromaDB query results.\n",
    "    \n",
    "    Args:\n",
    "        query_results: Dictionary from ChromaDB query (with ids, documents, metadatas, distances)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (context_text, source_documents)\n",
    "    \"\"\"\n",
    "    # Extract documents and metadata\n",
    "    documents = query_results['documents'][0]\n",
    "    metadatas = query_results['metadatas'][0]\n",
    "    distances = query_results['distances'][0]\n",
    "    \n",
    "    # Build context text\n",
    "    context_parts = []\n",
    "    source_documents = []\n",
    "    \n",
    "    for i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances), 1):\n",
    "        # Add document to context with reference number\n",
    "        context_parts.append(f\"[Document {i}]\")\n",
    "        context_parts.append(f\"Title: {metadata['title']}\")\n",
    "        context_parts.append(f\"Type: {metadata['type']}\")\n",
    "        context_parts.append(f\"Category: {metadata['category']}\")\n",
    "        context_parts.append(f\"Content: {doc}\")\n",
    "        context_parts.append(\"\")  # Empty line between documents\n",
    "        \n",
    "        # Store source information\n",
    "        source_documents.append({\n",
    "            'title': metadata['title'],\n",
    "            'type': metadata['type'],\n",
    "            'category': metadata['category'],\n",
    "            'distance': distance,\n",
    "            'link': metadata.get('link', 'N/A')\n",
    "        })\n",
    "    \n",
    "    context_text = \"\\n\".join(context_parts)\n",
    "    \n",
    "    return context_text, source_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai_with_context(\n",
    "    client,\n",
    "    question: str,\n",
    "    query_results: Dict[str, Any],\n",
    "    model: str = \"gpt-oss-120b\",\n",
    "    temperature: float = 0.1,\n",
    "    max_tokens: int = 1000\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Query OpenAI with context from ChromaDB results.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client object\n",
    "        question: User's question\n",
    "        query_results: Results from ChromaDB query\n",
    "        model: Model name (e.g., 'gpt-4', 'gpt-3.5-turbo')\n",
    "        temperature: Sampling temperature (0-2, lower is more focused)\n",
    "        max_tokens: Maximum tokens in response\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with answer and source documents\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare context from query results\n",
    "        context_text, source_documents = prepare_context_from_results(query_results)\n",
    "        print(context_text,source_documents)\n",
    "        \n",
    "        # Create system prompt\n",
    "        system_prompt = \"\"\"You are a helpful assistant that answers questions based ONLY on the provided context documents.\n",
    "\n",
    "IMPORTANT RULES:\n",
    "1. Answer ONLY using information from the provided documents and also if you see a content in an docs so try to create answer from the provided doc\n",
    "2. Do NOT use any external knowledge or information not present in the documents\n",
    "3. Try to create a answer from the given content its not necessary for exact answers, you can use your logical reasoning here\n",
    "4. Give answers ina n normal format do not use any stars or any extra special symbols in an answer\n",
    "\n",
    "\"\"\"\n",
    "# 3. If the answer is not in the provided documents, say \"I cannot find this information in the provided documents\"\n",
    "        # Create user prompt with context\n",
    "        user_prompt = f\"\"\"Context Documents:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please answer the question using ONLY the information from the context documents above. Do not use any external knowledge.\"\"\"\n",
    "\n",
    "        # Call OpenAI API\n",
    "        print(f\"\\nü§ñ Querying {model}...\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        # Extract answer\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Prepare result\n",
    "        result = {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'source_documents': source_documents,\n",
    "            'model': model,\n",
    "            'temperature': temperature,\n",
    "            'tokens_used': {\n",
    "                'prompt': response.usage.prompt_tokens,\n",
    "                'completion': response.usage.completion_tokens,\n",
    "                'total': response.usage.total_tokens\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"‚úì Response received successfully\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying OpenAI: {e}\")\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': f\"Error: {str(e)}\",\n",
    "            'source_documents': [],\n",
    "            'model': model\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7e34b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text = \"rules for disabled parking?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de8ebef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì OpenAI client created successfully\n",
      "  Base URL: https://api.ai.it.ufl.edu\n",
      "  Client type: <class 'openai.OpenAI'>\n",
      "  Has chat attribute: True\n",
      "  Has completions attribute: True\n",
      "[Document 1]\n",
      "Title: Parking\n",
      "Type: Regulation\n",
      "Category: Business Affairs\n",
      "Content: . (h) Unauthorized parking in Reserved Spaces or Restricted Areas is prohibited. (i) A Vehicle parked overtime at any time limited parking space (meters, time restricted loading zones and Service Drive Areas, etc.) may receive a citation at the time the violation is identified and may receive another citation in the same day if the Vehicle remains in the same space more than two (2) hours from the time of issuance of the first citation. (j) Vehicles may park according to Permit type in the appropriate lots and spaces as identified on the TAPS parking map and parking lot signage. (k) All Vehicle operators using a parking space controlled by a meter must pay to occupy the space in accordance with the instructions on the meter. (l) Only authorized Vehicles may park in disabled spaces\n",
      "\n",
      "[Document 2]\n",
      "Title: Parking\n",
      "Type: Regulation\n",
      "Category: Business Affairs\n",
      "Content: . (l) Only authorized Vehicles may park in disabled spaces. (m) Oversized Vehicles such as trucks, trailers, motor homes, or any Vehicle that occupies more than one (1) standard car space or extends beyond the space shall be parked in an area designated by TAPS with appropriate Permit. (n) Special Events/Maintenance: TAPS has authority to close streets, lots, and parking spaces to facilitate special events, and to perform necessary maintenance. Contact TAPS when planning a special event on campus to receive proper parking permits and assignments. No department has the authority to close any lots without first obtaining permission from TAPS. (5) Impounding Vehicles. (a) Vehicles are subject to being Impounded at the operator‚Äôs or owner‚Äôs expense under any of the following conditions: i. Unauthorized parking in Reserved Spaces, Restricted Areas, Service Drive Areas, no-parking zones, disabled spaces, or any other place in violation of this Regulation. ii\n",
      "\n",
      "[Document 3]\n",
      "Title: Parking\n",
      "Type: Regulation\n",
      "Category: Business Affairs\n",
      "Content: . (i) Disabled Parking: i. Students and Employees with a State-issued ‚ÄúDisabled Persons Parking Permit‚Äù or license plate must purchase a Permit in order to park on campus. ii. Visitors with a State-issued ‚ÄúDisabled Persons Parking Permit‚Äù or license plate may use designated disabled spaces and in non-reserved decal restricted spaces in order to park on campus. (j) Daily/Temporary Parking: i. Visitors may obtain a temporary Permit from TAPS; or utilize daily and hourly pay parking facilities or metered spaces upon payment of the required fee. ii. All vendors must be registered with UF, as visitor parking spaces may not be used for commercial purposes without prior approval. iii. UF departments or colleges sponsoring an event on campus shall schedule and reserve event parking with TAPS a minimum of two (2) weeks in advance of the event. TAPS will determine the assignment of event parking based on availability. (k) Permit Regulation: i\n",
      " [{'title': 'Parking', 'type': 'Regulation', 'category': 'Business Affairs', 'distance': 0.45503315329551697, 'link': 'N/A'}, {'title': 'Parking', 'type': 'Regulation', 'category': 'Business Affairs', 'distance': 0.4584954082965851, 'link': 'N/A'}, {'title': 'Parking', 'type': 'Regulation', 'category': 'Business Affairs', 'distance': 0.4655425548553467, 'link': 'N/A'}]\n",
      "\n",
      "ü§ñ Querying gpt-oss-120b...\n",
      "‚úì Response received successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = initialize_openai_client(\n",
    "    api_key=\"sk-pgNP-BIHOtI8RPt-aC3Stg\",\n",
    "    base_url='https://api.ai.it.ufl.edu'\n",
    ")\n",
    "query_result= query_chromadb(\n",
    "    collection=collection,\n",
    "    embedding_model=model,\n",
    "    query_text=question_text,\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "result = query_openai_with_context(\n",
    "    client=client,\n",
    "    question=question_text,\n",
    "    query_results=query_result,\n",
    "    model=\"gpt-oss-120b\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "585c1854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Only authorized vehicles may park in disabled spaces.  \\n- Authorization is provided by a State‚Äëissued ‚ÄúDisabled Persons Parking Permit‚Äù or a disabled‚Äëperson license plate.  \\n- Students and employees must purchase a campus permit in addition to the state‚Äëissued permit or plate before parking on campus.  \\n- Visitors who have a State‚Äëissued disabled permit or plate may use the designated disabled spaces (and non‚Äëreserved decal‚Äërestricted spaces) on campus.  \\n- Parking in a disabled space without the proper authorization is prohibited and can result in citations and possible impoundment of the vehicle.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875bcbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get answers of the questions from rag system for evaluation\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  \n",
    "\n",
    "def process_questions_from_excel(\n",
    "    excel_file,\n",
    "    collection,\n",
    "    embedding_model,\n",
    "    client,\n",
    "    question_column='question',\n",
    "    answer_column='rag_answer',\n",
    "    n_results=3,\n",
    "    llm_model=\"gpt-oss-120b\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    Process multiple questions from Excel and store RAG answers in new column\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    excel_file : str\n",
    "        Path to your Excel file\n",
    "    collection : ChromaDB collection\n",
    "        Your ChromaDB collection object\n",
    "    model : \n",
    "        Your embedding model\n",
    "    client : OpenAI client\n",
    "        Your OpenAI/compatible API client\n",
    "    question_column : str\n",
    "        Name of column containing questions (default: 'question')\n",
    "    answer_column : str\n",
    "        Name of new column to store answers (default: 'rag_answer')\n",
    "    n_results : int\n",
    "        Number of results from ChromaDB (default: 3)\n",
    "    gpt_model : str\n",
    "        Model name (default: \"gpt-oss-120b\")\n",
    "    temperature : float\n",
    "        Temperature for generation (default: 0.1)\n",
    "    max_tokens : int\n",
    "        Max tokens for response (default: 1000)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame with new answer column\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read Excel file\n",
    "    print(\"Reading Excel file: {excel_file}\")\n",
    "    df = pd.read_excel(excel_file)\n",
    "    print(f\"Loaded {len(df)} questions\")\n",
    "    \n",
    "    # Check if question column exists\n",
    "    if question_column not in df.columns:\n",
    "        raise ValueError(f\"Column '{question_column}' not found. Available columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Initialize answer column if it doesn't exist\n",
    "    if answer_column not in df.columns:\n",
    "        df[answer_column] = \"\"\n",
    "    \n",
    "    # Process each question\n",
    "    print(f\"\\n Processing questions...\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Getting answers\"):\n",
    "        question_text = str(row[question_column])\n",
    "        \n",
    "        # Skip if question is empty or NaN\n",
    "        if pd.isna(question_text) or question_text.strip() == \"\":\n",
    "            print(f\"  Skipping empty question at row {idx}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Query ChromaDB\n",
    "            query_result = query_chromadb(\n",
    "                collection=collection,\n",
    "                model=embedding_model,\n",
    "                query_text=question_text,\n",
    "                n_results=n_results\n",
    "            )\n",
    "            \n",
    "            # Get answer from OpenAI-compatible API\n",
    "            result = query_openai_with_context(\n",
    "                client=client,\n",
    "                question=question_text,\n",
    "                query_results=query_result,\n",
    "                model=llm_model,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens\n",
    "            )\n",
    "            \n",
    "            # Store answer in DataFrame\n",
    "            df.at[idx, answer_column] = result['answer']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Error processing question at row {idx}: {str(e)}\")\n",
    "            df.at[idx, answer_column] = f\"ERROR: {str(e)}\"\n",
    "    \n",
    "    # Save updated DataFrame back to Excel\n",
    "    output_file = excel_file.replace('.xlsx', '_with_answers.xlsx')\n",
    "    print(f\"\\n Saving results to: {output_file}\")\n",
    "    df.to_excel(output_file, index=False)\n",
    "    print(\"Done!\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54caafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Excel file\n",
    "\n",
    "##only change api key and model name to get new model\n",
    "#Try with different llms\n",
    "\"\"\"llama-3.1-70b-instruct\n",
    "mistral-7b-instruct\n",
    "gemma-3-27b-it\n",
    "gpt-oss-20b\n",
    "mistral-small-3.1\"\"\"\n",
    "new_api_key=\"sk-Jz7TVhKsaeJYJHRSaGl1ag\"\n",
    "client = initialize_openai_client(\n",
    "    api_key=\"sk-pgNP-BIHOtI8RPt-aC3Stg\",\n",
    "    base_url='https://api.ai.it.ufl.edu'\n",
    ")\n",
    "df_with_answers = process_questions_from_excel(\n",
    "    excel_file=\"your_questions.xlsx\",\n",
    "    collection=collection,\n",
    "    model=model,\n",
    "    client=client,\n",
    "    question_column='question', \n",
    "    answer_column='rag_answer',   \n",
    "    n_results=3,\n",
    "    llm_model=\"gpt-oss-120b\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "print(\"\\nFirst few results:\")\n",
    "print(df_with_answers[['question', 'rag_answer']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519e41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a68e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
