{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb23720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers chromadb pandas openpyxl tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764440a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(model_name=\"BAAI/bge-small-en-v1.5\"):\n",
    "    \"\"\"\n",
    "    Load BGE embedding model from HuggingFace.\n",
    "    \n",
    "    Available BGE models:\n",
    "    - BAAI/bge-small-en-v1.5 (fastest, 384 dimensions)\n",
    "    - BAAI/bge-base-en-v1.5 (balanced, 768 dimensions)\n",
    "    - BAAI/bge-large-en-v1.5 (best quality, 1024 dimensions)\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model identifier\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer model\n",
    "    \"\"\"\n",
    "    print(f\"Loading embedding model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(f\" Model loaded successfully!\")\n",
    "    print(f\"  Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_data(excel_file):\n",
    "    \"\"\"Load policy data from Excel file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(excel_file, sheet_name='Policies')\n",
    "        print(f\" Loaded {len(df)} rows from {excel_file}\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Excel: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886a49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chromadb(persist_directory=\"./chroma_db\", collection_name=\"policies\"):\n",
    "    \"\"\"\n",
    "    Initialize ChromaDB client and create/get collection.\n",
    "    \n",
    "    Args:\n",
    "        persist_directory: Local directory to persist the database\n",
    "        collection_name: Name of the collection\n",
    "    \n",
    "    Returns:\n",
    "        ChromaDB collection object\n",
    "    \"\"\"\n",
    "    # Create persist directory if it doesn't exist\n",
    "    os.makedirs(persist_directory, exist_ok=True)\n",
    "    \n",
    "    # Initialize ChromaDB client with persistence\n",
    "    client = chromadb.PersistentClient(path=persist_directory)\n",
    "    \n",
    "    print(f\"ChromaDB initialized at: {persist_directory}\")\n",
    "    \n",
    "    # Get or create collection\n",
    "    try:\n",
    "        # Try to get existing collection\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "        print(f\" Retrieved existing collection: {collection_name}\")\n",
    "        print(f\"  Current documents: {collection.count()}\")\n",
    "    except:\n",
    "        # Create new collection if doesn't exist\n",
    "        collection = client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": \"UF Policy documents with embeddings\"}\n",
    "        )\n",
    "        print(f\" Created new collection: {collection_name}\")\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_batch(model, texts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create embeddings for a list of texts in batches.\n",
    "    \n",
    "    Args:\n",
    "        model: SentenceTransformer model\n",
    "        texts: List of text strings\n",
    "        batch_size: Number of texts to process at once\n",
    "    \n",
    "    Returns:\n",
    "        List of embeddings\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Creating embeddings\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch_embeddings = model.encode(batch, show_progress_bar=False)\n",
    "        embeddings.extend(batch_embeddings.tolist())\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab130606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_chromadb(collection, df, embeddings, include_types=None):\n",
    "    \"\"\"\n",
    "    Store documents with embeddings in ChromaDB.\n",
    "    \n",
    "    Args:\n",
    "        collection: ChromaDB collection\n",
    "        df: Pandas DataFrame with policy data\n",
    "        embeddings: List of embeddings\n",
    "        include_types: List of policy types to include (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        Number of documents stored\n",
    "    \"\"\"\n",
    "    # Filter by type if specified\n",
    "    if include_types:\n",
    "        df = df[df['type'].isin(include_types)]\n",
    "        print(f\"Filtering by types: {include_types}\")\n",
    "        print(f\"Documents after filtering: {len(df)}\")\n",
    "    \n",
    "    # Prepare data for ChromaDB\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    filtered_embeddings = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Create unique ID\n",
    "        doc_id = f\"{row['title']}_{row['chunk_number']}\".replace(\" \", \"_\")\n",
    "        \n",
    "        # Document text\n",
    "        documents.append(row['content'])\n",
    "        \n",
    "        # Metadata\n",
    "        metadatas.append({\n",
    "            'title': str(row['title']),\n",
    "            'category': str(row['category']),\n",
    "            'type': str(row['type']),\n",
    "            # 'link': str(row['link']),\n",
    "            # 'chunk_number': int(row['chunk_number']),\n",
    "            # 'total_chunks': int(row['total_chunks']),\n",
    "            # 'chunk_size': int(row['chunk_size'])\n",
    "        })\n",
    "        \n",
    "        # IDs\n",
    "        ids.append(f\"{doc_id}_{idx}\")\n",
    "        \n",
    "        # Corresponding embedding\n",
    "        filtered_embeddings.append(embeddings[idx])\n",
    "    \n",
    "    # Store in ChromaDB in batches\n",
    "    batch_size = 100\n",
    "    total_stored = 0\n",
    "    \n",
    "    for i in tqdm(range(0, len(documents), batch_size), desc=\"Storing in ChromaDB\"):\n",
    "        batch_end = min(i + batch_size, len(documents))\n",
    "        \n",
    "        collection.add(\n",
    "            documents=documents[i:batch_end],\n",
    "            embeddings=filtered_embeddings[i:batch_end],\n",
    "            metadatas=metadatas[i:batch_end],\n",
    "            ids=ids[i:batch_end]\n",
    "        )\n",
    "        \n",
    "        total_stored += (batch_end - i)\n",
    "    \n",
    "    print(f\"Stored {total_stored} documents in ChromaDB\")\n",
    "    return total_stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c644c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb(collection, model, query_text, n_results=5, filter_by_type=None):\n",
    "    \"\"\"\n",
    "    Query ChromaDB with a text query.\n",
    "    \n",
    "    Args:\n",
    "        collection: ChromaDB collection\n",
    "        model: SentenceTransformer model for query embedding\n",
    "        query_text: Query string\n",
    "        n_results: Number of results to return\n",
    "        filter_by_type: Filter results by policy type (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Query results\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = model.encode([query_text])[0].tolist()\n",
    "    \n",
    "    # Prepare filter\n",
    "    where_filter = None\n",
    "    if filter_by_type:\n",
    "        where_filter = {\"type\": {\"$eq\": filter_by_type}}\n",
    "    \n",
    "    # Query ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        where=where_filter\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9815229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb(collection, model, query_text, n_results=5, filter_by_type=None):\n",
    "    \"\"\"\n",
    "    Query ChromaDB with a text query.\n",
    "    \n",
    "    Args:\n",
    "        collection: ChromaDB collection\n",
    "        model: SentenceTransformer model for query embedding\n",
    "        query_text: Query string\n",
    "        n_results: Number of results to return\n",
    "        filter_by_type: Filter results by policy type (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Query results\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = model.encode([query_text])[0].tolist()\n",
    "    \n",
    "    # Prepare filter\n",
    "    where_filter = None\n",
    "    if filter_by_type:\n",
    "        where_filter = {\"type\": {\"$eq\": filter_by_type}}\n",
    "    \n",
    "    # Query ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        where=where_filter\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646606d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(results):\n",
    "    \"\"\"Display query results in a readable format.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QUERY RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, (doc, metadata, distance) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    ), 1):\n",
    "        print(f\"\\n Result {i} (Distance: {distance:.4f})\")\n",
    "        print(f\"Title: {metadata['title']}\")\n",
    "        print(f\"Category: {metadata['category']} | Type: {metadata['type']}\")\n",
    "        # print(f\"Chunk: {metadata['chunk_number']}/{metadata['total_chunks']}\")\n",
    "        # print(f\"Link: {metadata['link']}\")\n",
    "        print(f\"\\nContent Preview:\")\n",
    "        print(f\"{doc[:300]}...\")\n",
    "        print(\"-\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(excel_file, \n",
    "                  persist_directory=\"./chroma_db\",\n",
    "                  collection_name=\"policies\",\n",
    "                  model_name=\"BAAI/bge-small-en-v1.5\",\n",
    "                  include_types=None,\n",
    "                  overwrite=False):\n",
    "    \"\"\"\n",
    "    Main pipeline to load data, create embeddings, and store in ChromaDB.\n",
    "    \n",
    "    Args:\n",
    "        excel_file: Path to Excel file\n",
    "        persist_directory: ChromaDB storage directory\n",
    "        collection_name: Name of ChromaDB collection\n",
    "        model_name: BGE model name\n",
    "        include_types: List of types to include (e.g., ['Policy', 'Regulation'])\n",
    "        overwrite: If True, delete existing collection and create new one\n",
    "    \n",
    "    Returns:\n",
    "        collection: ChromaDB collection object\n",
    "        model: Embedding model\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Step 2: Load embedding model\n",
    "    print(f\"\\nStep 2: Loading embedding model...\")\n",
    "    model = load_embedding_model(model_name)\n",
    "    \n",
    "    # Step 3: Initialize ChromaDB\n",
    "    print(f\"\\nStep 3: Initializing ChromaDB...\")\n",
    "    if overwrite:\n",
    "        client = chromadb.PersistentClient(path=persist_directory)\n",
    "        try:\n",
    "            client.delete_collection(name=collection_name)\n",
    "            print(f\" Deleted existing collection: {collection_name}\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    collection = initialize_chromadb(persist_directory, collection_name)\n",
    "    \n",
    "\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" PIPELINE COMPLETE!\")\n",
    "    print(f\" Total documents in collection: {collection.count()}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return collection, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2e85d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Loading embedding model...\n",
      "   ðŸ¤— Loading model from HuggingFace: BAAI/bge-base-en-v1.5\n",
      "\n",
      "Step 3: Initializing ChromaDB...\n",
      "âœ“ ChromaDB initialized at: ./chroma_db\n",
      "âœ“ Retrieved existing collection: policies\n",
      "  Current documents: 1887\n",
      "\n",
      "================================================================================\n",
      "âœ“ PIPELINE COMPLETE!\n",
      "âœ“ Total documents in collection: 1887\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 1. Basic usage - store all policies\n",
    "# 3. Use different BGE model\n",
    "collection, model = main_pipeline(\n",
    "    excel_file=\"policies_data.xlsx\",\n",
    "    model_name=\"BAAI/bge-base-en-v1.5\"  # Better quality\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8731fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QUERY RESULTS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“„ Result 1 (Distance: 0.5600)\n",
      "Title: Working Safely and Maintaining Workplace Health Standards\n",
      "Category: Human Resources | Type: Policy\n",
      "\n",
      "Content Preview:\n",
      ". Employees should not return to work until symptoms have improved. Employees must follow all department reporting procedures for absences, including accurately reporting their work time and absence from the office in myUFL. During the absence, employees may work from home, if approved, and shall us...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“„ Result 2 (Distance: 0.5679)\n",
      "Title: Furlough Policy (includes UFF Faculty MOU)\n",
      "Category: Human Resources | Type: Policy\n",
      "\n",
      "Content Preview:\n",
      ". Policy Statement A furlough is a mandatory unpaid partial or full leave of absence from work. The University shall consider the availability and feasibility of other costâ€saving measures before implementing furloughs...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ“„ Result 3 (Distance: 0.5808)\n",
      "Title: Paid Family Leave\n",
      "Category: Human Resources | Type: Policy\n",
      "\n",
      "Content Preview:\n",
      ". 4.3 Paid Medical Leave Eligibility for Employee Illness/Injury Eligibility An employee must qualify for and go on continuous FMLA leave A healthcare provider certifies that the period of continuous medically necessary absence will be at least three (3) or more weeks (15 or more working days) for t...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 5. Query with type filter\n",
    "results = query_chromadb(\n",
    "    collection=collection,\n",
    "    model=model,\n",
    "    query_text=\"leave and absence policy\",\n",
    "    n_results=3,\n",
    "    filter_by_type=\"Policy\"  # Only search Policy type\n",
    ")\n",
    "\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "144a59e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['Working_Safely_and_Maintaining_Workplace_Health_Standards_3_1520',\n",
       "   'Furlough_Policy_(includes_UFF_Faculty_MOU)_2_1853',\n",
       "   'Paid_Family_Leave_7_1595']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['. Employees should not return to work until symptoms have improved. Employees must follow all department reporting procedures for absences, including accurately reporting their work time and absence from the office in myUFL. During the absence, employees may work from home, if approved, and shall use accrued sick leave or leave without pay if unable to work. In situations in which the reason for the absence is due to an employeeâ€™s serious health condition, the absence may be covered by the Family Medical Leave Act (FMLA) . Employees must practice and maintain personal hygiene to help ensure a healthy workplace. Strategies to accomplish this include: Washing hands to aid in keeping employees healthy and preventing the spread of illness to co-workers. All employees should adhere to the CDC recommendations on handwashing: Wash your hands often with soap and water for at least 20 seconds especially after you have been in a public place or after blowing your nose, coughing or sneezing',\n",
       "   '. Policy Statement A furlough is a mandatory unpaid partial or full leave of absence from work. The University shall consider the availability and feasibility of other costâ€saving measures before implementing furloughs',\n",
       "   '. 4.3 Paid Medical Leave Eligibility for Employee Illness/Injury Eligibility An employee must qualify for and go on continuous FMLA leave A healthcare provider certifies that the period of continuous medically necessary absence will be at least three (3) or more weeks (15 or more working days) for the employeeâ€™s serious illness or injury The illness or injury prevents the employee from performing the material and substantial duties of their UF position The illness or injury prevents the employee from performing majority of activities of daily living The employee must be under the ongoing care of a Physician in the appropriate specialty as determined by UF Central Leave']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'type': 'Policy',\n",
       "    'title': 'Working Safely and Maintaining Workplace Health Standards',\n",
       "    'category': 'Human Resources'},\n",
       "   {'title': 'Furlough Policy (includes UFF Faculty MOU)',\n",
       "    'type': 'Policy',\n",
       "    'category': 'Human Resources'},\n",
       "   {'title': 'Paid Family Leave',\n",
       "    'type': 'Policy',\n",
       "    'category': 'Human Resources'}]],\n",
       " 'distances': [[0.5599740743637085, 0.5678935050964355, 0.5807972550392151]]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7fa743",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "\n",
    "\n",
    "\n",
    "####Retrival pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eed570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "def initialize_openai_client(api_key: str, base_url: str = 'https://api.ai.it.ufl.edu'):\n",
    "    \"\"\"\n",
    "    Initialize OpenAI client with custom base URL.\n",
    "    \n",
    "    Args:\n",
    "        api_key: Your OpenAI API key\n",
    "        base_url: Custom base URL for API (default: UF API endpoint)\n",
    "    \n",
    "    Returns:\n",
    "        OpenAI client object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=base_url\n",
    "        )\n",
    "        \n",
    "        print('âœ“ OpenAI client created successfully')\n",
    "        print(f'  Base URL: {base_url}')\n",
    "        print(f'  Client type: {type(client)}')\n",
    "        \n",
    "        # Sanity checks\n",
    "        has_chat = hasattr(client, 'chat')\n",
    "        print(f'  Has chat attribute: {has_chat}')\n",
    "        \n",
    "        if has_chat:\n",
    "            has_completions = hasattr(client.chat, 'completions')\n",
    "            print(f'  Has completions attribute: {has_completions}')\n",
    "        \n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f' Error initializing OpenAI client: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "614f59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "def prepare_context_from_results(query_results: Dict[str, Any]) -> tuple:\n",
    "    \"\"\"\n",
    "    Prepare context and metadata from ChromaDB query results.\n",
    "    \n",
    "    Args:\n",
    "        query_results: Dictionary from ChromaDB query (with ids, documents, metadatas, distances)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (context_text, source_documents)\n",
    "    \"\"\"\n",
    "    # Extract documents and metadata\n",
    "    documents = query_results['documents'][0]\n",
    "    metadatas = query_results['metadatas'][0]\n",
    "    distances = query_results['distances'][0]\n",
    "    \n",
    "    # Build context text\n",
    "    context_parts = []\n",
    "    source_documents = []\n",
    "    \n",
    "    for i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances), 1):\n",
    "        # Add document to context with reference number\n",
    "        context_parts.append(f\"[Document {i}]\")\n",
    "        context_parts.append(f\"Title: {metadata['title']}\")\n",
    "        context_parts.append(f\"Type: {metadata['type']}\")\n",
    "        context_parts.append(f\"Category: {metadata['category']}\")\n",
    "        context_parts.append(f\"Content: {doc}\")\n",
    "        context_parts.append(\"\")  # Empty line between documents\n",
    "        \n",
    "        # Store source information\n",
    "        source_documents.append({\n",
    "            'title': metadata['title'],\n",
    "            'type': metadata['type'],\n",
    "            'category': metadata['category'],\n",
    "            'distance': distance,\n",
    "            'link': metadata.get('link', 'N/A')\n",
    "        })\n",
    "    \n",
    "    context_text = \"\\n\".join(context_parts)\n",
    "    \n",
    "    return context_text, source_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai_with_context(\n",
    "    client,\n",
    "    question: str,\n",
    "    query_results: Dict[str, Any],\n",
    "    model: str = \"gpt-oss-120b\",\n",
    "    temperature: float = 0.1,\n",
    "    max_tokens: int = 1000\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Query OpenAI with context from ChromaDB results.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client object\n",
    "        question: User's question\n",
    "        query_results: Results from ChromaDB query\n",
    "        model: Model name (e.g., 'gpt-4', 'gpt-3.5-turbo')\n",
    "        temperature: Sampling temperature (0-2, lower is more focused)\n",
    "        max_tokens: Maximum tokens in response\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with answer and source documents\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare context from query results\n",
    "        context_text, source_documents = prepare_context_from_results(query_results)\n",
    "        \n",
    "        # Create system prompt\n",
    "        system_prompt = \"\"\"You are a helpful assistant that answers questions based ONLY on the provided context documents.\n",
    "\n",
    "IMPORTANT RULES:\n",
    "1. Answer ONLY using information from the provided documents\n",
    "2. Do NOT use any external knowledge or information not present in the documents\n",
    "3. If the answer is not in the provided documents, say \"I cannot find this information in the provided documents\"\n",
    "4. Cite which document(s) you used by referring to [Document N] format\n",
    "5. Be accurate and concise in your response\"\"\"\n",
    "\n",
    "        # Create user prompt with context\n",
    "        user_prompt = f\"\"\"Context Documents:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please answer the question using ONLY the information from the context documents above. Do not use any external knowledge.\"\"\"\n",
    "\n",
    "        # Call OpenAI API\n",
    "        print(f\"\\n Querying {model}...\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        # Extract answer\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Prepare result\n",
    "        result = {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'source_documents': source_documents,\n",
    "            'model': model,\n",
    "            'temperature': temperature,\n",
    "            'tokens_used': {\n",
    "                'prompt': response.usage.prompt_tokens,\n",
    "                'completion': response.usage.completion_tokens,\n",
    "                'total': response.usage.total_tokens\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"âœ“ Response received successfully\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error querying OpenAI: {e}\")\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': f\"Error: {str(e)}\",\n",
    "            'source_documents': [],\n",
    "            'model': model\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d27b7c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ OpenAI client created successfully\n",
      "  Base URL: https://api.ai.it.ufl.edu\n",
      "  Client type: <class 'openai.OpenAI'>\n",
      "  Has chat attribute: True\n",
      "  Has completions attribute: True\n",
      "\n",
      "ðŸ¤– Querying gpt-oss-120b...\n",
      "âœ“ Response received successfully\n"
     ]
    }
   ],
   "source": [
    "client = initialize_openai_client(\n",
    "    api_key=\"sk-pgNP-BIHOtI8RPt-aC3Stg\",\n",
    "    base_url='https://api.ai.it.ufl.edu'\n",
    ")\n",
    "\n",
    "\n",
    "result = query_openai_with_context(\n",
    "    client=client,\n",
    "    question=\"research asistant polic?\",\n",
    "    query_results=results,\n",
    "    model=\"gpt-oss-120b\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36c651b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'research asistant polic?',\n",
       " 'answer': 'I cannot find this information in the provided documents.',\n",
       " 'source_documents': [{'title': 'Working Safely and Maintaining Workplace Health Standards',\n",
       "   'type': 'Policy',\n",
       "   'category': 'Human Resources',\n",
       "   'distance': 0.5599740743637085,\n",
       "   'link': 'N/A'},\n",
       "  {'title': 'Furlough Policy (includes UFF Faculty MOU)',\n",
       "   'type': 'Policy',\n",
       "   'category': 'Human Resources',\n",
       "   'distance': 0.5678935050964355,\n",
       "   'link': 'N/A'},\n",
       "  {'title': 'Paid Family Leave',\n",
       "   'type': 'Policy',\n",
       "   'category': 'Human Resources',\n",
       "   'distance': 0.5807972550392151,\n",
       "   'link': 'N/A'}],\n",
       " 'model': 'gpt-oss-120b',\n",
       " 'temperature': 0.1,\n",
       " 'tokens_used': {'prompt': 630, 'completion': 65, 'total': 695}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b348cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
