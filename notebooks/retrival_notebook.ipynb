{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ad5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Dict, List, Any\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a6327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d25e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(model_name=\"BAAI/bge-small-en-v1.5\"):\n",
    "    \"\"\"\n",
    "    Load BGE embedding model from HuggingFace.\n",
    "    \n",
    "    Available BGE models:\n",
    "    - BAAI/bge-small-en-v1.5 (fastest, 384 dimensions)\n",
    "    - BAAI/bge-base-en-v1.5 (balanced, 768 dimensions)\n",
    "    - BAAI/bge-large-en-v1.5 (best quality, 1024 dimensions)\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model identifier\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer model\n",
    "    \"\"\"\n",
    "    print(f\"Loading embedding model: {model_name}\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(f\"✓ Model loaded successfully!\")\n",
    "    print(f\"  Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfaa7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chromadb(persist_directory=\"./chroma_db\", collection_name=\"policies\"):\n",
    "    \"\"\"\n",
    "    Initialize ChromaDB client and create/get collection.\n",
    "    \n",
    "    Args:\n",
    "        persist_directory: Local directory to persist the database\n",
    "        collection_name: Name of the collection\n",
    "    \n",
    "    Returns:\n",
    "        ChromaDB collection object\n",
    "    \"\"\"\n",
    "    # Create persist directory if it doesn't exist\n",
    "    os.makedirs(persist_directory, exist_ok=True)\n",
    "    \n",
    "    # Initialize ChromaDB client with persistence\n",
    "    client = chromadb.PersistentClient(path=persist_directory)\n",
    "    \n",
    "    print(f\"✓ ChromaDB initialized at: {persist_directory}\")\n",
    "    \n",
    "    # Get or create collection\n",
    "    try:\n",
    "        # Try to get existing collection\n",
    "        collection = client.get_collection(name=collection_name)\n",
    "        print(f\"✓ Retrieved existing collection: {collection_name}\")\n",
    "        print(f\"  Current documents: {collection.count()}\")\n",
    "    except:\n",
    "        # Create new collection if doesn't exist\n",
    "        collection = client.create_collection(\n",
    "            name=collection_name,\n",
    "            metadata={\"description\": \"UF Policy documents with embeddings\"}\n",
    "        )\n",
    "        print(f\"✓ Created new collection: {collection_name}\")\n",
    "    \n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa4ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_openai_client(api_key: str, base_url: str = 'https://api.ai.it.ufl.edu'):\n",
    "    \"\"\"\n",
    "    Initialize OpenAI client with custom base URL.\n",
    "    \n",
    "    Args:\n",
    "        api_key: Your OpenAI API key\n",
    "        base_url: Custom base URL for API (default: UF API endpoint)\n",
    "    \n",
    "    Returns:\n",
    "        OpenAI client object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = openai.OpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=base_url\n",
    "        )\n",
    "        \n",
    "        print('✓ OpenAI client created successfully')\n",
    "        print(f'  Base URL: {base_url}')\n",
    "        print(f'  Client type: {type(client)}')\n",
    "        \n",
    "        # Sanity checks\n",
    "        has_chat = hasattr(client, 'chat')\n",
    "        print(f'  Has chat attribute: {has_chat}')\n",
    "        \n",
    "        if has_chat:\n",
    "            has_completions = hasattr(client.chat, 'completions')\n",
    "            print(f'  Has completions attribute: {has_completions}')\n",
    "        \n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f'Error initializing OpenAI client: {e}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1993ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chromadb(collection, model, query_text, n_results=5, filter_by_type=None):\n",
    "    \"\"\"\n",
    "    Query ChromaDB with a text query.\n",
    "    \n",
    "    Args:\n",
    "        collection: ChromaDB collection\n",
    "        model: SentenceTransformer model for query embedding\n",
    "        query_text: Query string\n",
    "        n_results: Number of results to return\n",
    "        filter_by_type: Filter results by policy type (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Query results\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = model.encode([query_text])[0].tolist()\n",
    "    \n",
    "    # Prepare filter\n",
    "    where_filter = None\n",
    "    if filter_by_type:\n",
    "        where_filter = {\"type\": {\"$eq\": filter_by_type}}\n",
    "    \n",
    "    # Query ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        where=where_filter\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "565efa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context_from_results(query_results: Dict[str, Any]) -> tuple:\n",
    "    \"\"\"\n",
    "    Prepare context and metadata from ChromaDB query results.\n",
    "    \n",
    "    Args:\n",
    "        query_results: Dictionary from ChromaDB query (with ids, documents, metadatas, distances)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (context_text, source_documents)\n",
    "    \"\"\"\n",
    "    # Extract documents and metadata\n",
    "    documents = query_results['documents'][0]\n",
    "    metadatas = query_results['metadatas'][0]\n",
    "    distances = query_results['distances'][0]\n",
    "    \n",
    "    # Build context text\n",
    "    context_parts = []\n",
    "    source_documents = []\n",
    "    \n",
    "    for i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances), 1):\n",
    "        # Add document to context with reference number\n",
    "        context_parts.append(f\"[Document {i}]\")\n",
    "        context_parts.append(f\"Title: {metadata['title']}\")\n",
    "        context_parts.append(f\"Type: {metadata['type']}\")\n",
    "        context_parts.append(f\"Category: {metadata['category']}\")\n",
    "        context_parts.append(f\"Content: {doc}\")\n",
    "        context_parts.append(\"\")  # Empty line between documents\n",
    "        \n",
    "        # Store source information\n",
    "        source_documents.append({\n",
    "            'title': metadata['title'],\n",
    "            'type': metadata['type'],\n",
    "            'category': metadata['category'],\n",
    "            'distance': distance,\n",
    "            'link': metadata.get('link', 'N/A')\n",
    "        })\n",
    "    \n",
    "    context_text = \"\\n\".join(context_parts)\n",
    "    \n",
    "    return context_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb35d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_openai_with_context(\n",
    "    client,\n",
    "    question: str,\n",
    "    query_results: Dict[str, Any],\n",
    "    model: str = \"gpt-oss-120b\",\n",
    "    temperature: float = 0.1,\n",
    "    max_tokens: int = 1000\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Query OpenAI with context from ChromaDB results.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client object\n",
    "        question: User's question\n",
    "        query_results: Results from ChromaDB query\n",
    "        model: Model name (e.g., 'gpt-4', 'gpt-3.5-turbo')\n",
    "        temperature: Sampling temperature (0-2, lower is more focused)\n",
    "        max_tokens: Maximum tokens in response\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with answer and source documents\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare context from query results\n",
    "        context_text = prepare_context_from_results(query_results)\n",
    "        # print(context_text,source_documents)\n",
    "        \n",
    "        # Create system prompt\n",
    "        system_prompt = \"\"\"You are a helpful assistant that answers questions based ONLY on the provided context documents text.\n",
    "\n",
    "IMPORTANT RULES:\n",
    "1. Answer ONLY using information from the provided documents and also if you see a content in an docs so try to create answer from the provided doc content.\n",
    "2. Do NOT use any external knowledge or information not present in the documents text provided\n",
    "3. Try to create a answer from the given content its not necessary for exact answers, you can use your logical reasoning here to frame the answers.\n",
    "4. Give answers in an normal format do not use any stars or any extra special symbols in an answer also dont give e blank lines in the response\n",
    "\n",
    "\"\"\"\n",
    "# 3. If the answer is not in the provided documents, say \"I cannot find this information in the provided documents\"\n",
    "        # Create user prompt with context\n",
    "        user_prompt = f\"\"\"Context Documents Content:\n",
    "{context_text}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please answer the question using ONLY the information from the context content above. Do not use any external knowledge.\"\"\"\n",
    "\n",
    "        # Call OpenAI API\n",
    "        print(f\"\\n Querying {model}...\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        # Extract answer\n",
    "        answer = response.choices[0].message.content\n",
    "        \n",
    "        # Prepare result\n",
    "        result = {\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            # 'source_documents': source_documents,\n",
    "            'model': model,\n",
    "            'temperature': temperature,\n",
    "            'tokens_used': {\n",
    "                'prompt': response.usage.prompt_tokens,\n",
    "                'completion': response.usage.completion_tokens,\n",
    "                'total': response.usage.total_tokens\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"✓ Response received successfully\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error querying OpenAI: {e}\")\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': f\"Error: {str(e)}\",\n",
    "            'source_documents': [],\n",
    "            'model': model\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75b6b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline(\n",
    "                  persist_directory=\"./chroma_db\",\n",
    "                  collection_name=\"policies\",\n",
    "                  model_name=\"BAAI/bge-base-en-v1.5\",\n",
    "                  include_types=None,\n",
    "                  ):\n",
    "    \"\"\"\n",
    "    Main pipeline to load data, create embeddings, and store in ChromaDB.\n",
    "    \n",
    "    Args:\n",
    "        excel_file: Path to Excel file\n",
    "        persist_directory: ChromaDB storage directory\n",
    "        collection_name: Name of ChromaDB collection\n",
    "        model_name: BGE model name\n",
    "        include_types: List of types to include (e.g., ['Policy', 'Regulation'])\n",
    "        overwrite: If True, delete existing collection and create new one\n",
    "    \n",
    "    Returns:\n",
    "        collection: ChromaDB collection object\n",
    "        model: Embedding model\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Step 2: Load embedding model\n",
    "    print(f\"\\nStep 2: Loading embedding model...\")\n",
    "    model = load_embedding_model(model_name)\n",
    "    \n",
    "    # Step 3: Initialize ChromaDB\n",
    "    print(f\"\\nStep 3: Initializing ChromaDB...\")\n",
    "    \n",
    "    collection = initialize_chromadb(persist_directory, collection_name)\n",
    "    # Step 4 initialize Openai client\n",
    "    client = initialize_openai_client(\n",
    "    api_key=\"sk-pgNP-BIHOtI8RPt-aC3Stg\",\n",
    "    base_url='https://api.ai.it.ufl.edu'\n",
    ")\n",
    "    \n",
    "    \n",
    "    return collection,model,client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46f89175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Loading embedding model...\n",
      "Loading embedding model: BAAI/bge-base-en-v1.5\n",
      "✓ Model loaded successfully!\n",
      "  Embedding dimension: 768\n",
      "\n",
      "Step 3: Initializing ChromaDB...\n",
      "✓ ChromaDB initialized at: ./chroma_db\n",
      "✓ Retrieved existing collection: policies\n",
      "  Current documents: 1887\n",
      "✓ OpenAI client created successfully\n",
      "  Base URL: https://api.ai.it.ufl.edu\n",
      "  Client type: <class 'openai.OpenAI'>\n",
      "  Has chat attribute: True\n",
      "  Has completions attribute: True\n"
     ]
    }
   ],
   "source": [
    "collection,model,client=main_pipeline()\n",
    "# question_text = \"rules for disabled parking?\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0e1276c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Querying gpt-oss-120b...\n",
      "✓ Response received successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The University of Florida expressly prohibits sexual harassment.  Complaints that allege sexual harassment or sexual misconduct must be filed with the Director of Employee and Labor Relations or the University Title\\u202fIX Coordinator and are processed in accordance with the university’s Policy on Sexual Harassment.  That policy (referenced in the Title\\u202fIX Policy) defines the prohibited conduct and sets out the steps UF will follow to investigate and resolve formal complaints, including the range of possible resolution options.  While the specific penalties are not listed in the excerpts, the university’s general misconduct policy states that substantiated acts of violence, threats, and aggression can result in disciplinary action up to and including termination, and the same disciplinary framework is applied to violations of the sexual‑harassment policy.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_text = \"Sexual harasment rules and punishment in uf?\"\n",
    "\n",
    "query_result= query_chromadb(\n",
    "collection=collection,\n",
    "model=model,\n",
    "query_text=question_text,\n",
    "n_results=3,\n",
    "\n",
    ")\n",
    "result = query_openai_with_context(\n",
    "client=client,\n",
    "question=question_text,\n",
    "query_results=query_result,\n",
    "model=\"gpt-oss-120b\",\n",
    "temperature=0.1,\n",
    "max_tokens=1000\n",
    ")\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5419f4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The University of Florida expressly prohibits sexual harassment.  Complaints that allege sexual harassment or sexual misconduct must be filed with the Director of Employee and Labor Relations or the University Title IX Coordinator and are processed in accordance with the university’s Policy on Sexual Harassment.  That policy (referenced in the Title IX Policy) defines the prohibited conduct and sets out the steps UF will follow to investigate and resolve formal complaints, including the range of possible resolution options.  While the specific penalties are not listed in the excerpts, the university’s general misconduct policy states that substantiated acts of violence, threats, and aggression can result in disciplinary action up to and including termination, and the same disciplinary framework is applied to violations of the sexual‑harassment policy.\n"
     ]
    }
   ],
   "source": [
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0de53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da38b4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
