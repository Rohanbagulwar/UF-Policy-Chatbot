{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3923d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "from datasets import Dataset\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize LLM for RAGAS (you can use OpenAI or other providers)\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "def calculate_bleu_score(reference, candidate):\n",
    "    \"\"\"Calculate BLEU score between reference and candidate answers\"\"\"\n",
    "    try:\n",
    "        reference_tokens = reference.lower().split()\n",
    "        candidate_tokens = candidate.lower().split()\n",
    "        \n",
    "        # Use smoothing function to avoid zero scores\n",
    "        smoothie = SmoothingFunction().method4\n",
    "        score = sentence_bleu([reference_tokens], candidate_tokens, \n",
    "                             smoothing_function=smoothie)\n",
    "        return round(score, 4)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def evaluate_answers(excel_file, sheet_name='Sheet1'):\n",
    "    \"\"\"\n",
    "    Evaluate answers from Excel sheet with multiple metrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    excel_file : str\n",
    "        Path to Excel file\n",
    "    sheet_name : str\n",
    "        Name of the sheet to read\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read Excel file\n",
    "    df = pd.read_excel(excel_file, sheet_name=sheet_name)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} rows from Excel\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Initialize result columns\n",
    "    df['BLEU_GPT_vs_Human'] = 0.0\n",
    "    df['BLEU_OSS_vs_Human'] = 0.0\n",
    "    df['BLEU_RAG_vs_Human'] = 0.0\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    print(\"\\nCalculating BLEU scores...\")\n",
    "    for idx, row in df.iterrows():\n",
    "        human_answer = str(row['human_answer'])\n",
    "        \n",
    "        if 'gpt_answer' in df.columns:\n",
    "            df.at[idx, 'BLEU_GPT_vs_Human'] = calculate_bleu_score(\n",
    "                human_answer, str(row['gpt_answer'])\n",
    "            )\n",
    "        \n",
    "        if 'oss_answer' in df.columns:\n",
    "            df.at[idx, 'BLEU_OSS_vs_Human'] = calculate_bleu_score(\n",
    "                human_answer, str(row['oss_answer'])\n",
    "            )\n",
    "        \n",
    "        if 'rag_answer' in df.columns:\n",
    "            df.at[idx, 'BLEU_RAG_vs_Human'] = calculate_bleu_score(\n",
    "                human_answer, str(row['rag_answer'])\n",
    "            )\n",
    "    \n",
    "    # Prepare data for RAGAS evaluation\n",
    "    print(\"\\nPreparing data for RAGAS evaluation...\")\n",
    "    \n",
    "    # For GPT answers\n",
    "    if 'gpt_answer' in df.columns and 'question' in df.columns:\n",
    "        gpt_dataset = Dataset.from_dict({\n",
    "            'question': df['question'].tolist(),\n",
    "            'answer': df['gpt_answer'].tolist(),\n",
    "            'ground_truth': df['human_answer'].tolist(),\n",
    "            'contexts': [[str(ans)] for ans in df['gpt_answer'].tolist()]\n",
    "        })\n",
    "        \n",
    "        print(\"\\nEvaluating GPT answers with RAGAS...\")\n",
    "        gpt_results = evaluate(\n",
    "            gpt_dataset,\n",
    "            metrics=[answer_correctness, answer_similarity, answer_relevancy],\n",
    "            llm=llm,\n",
    "            embeddings=embeddings\n",
    "        )\n",
    "        \n",
    "        gpt_df = gpt_results.to_pandas()\n",
    "        df['RAGAS_GPT_Correctness'] = gpt_df['answer_correctness'].values\n",
    "        df['RAGAS_GPT_Similarity'] = gpt_df['answer_similarity'].values\n",
    "        df['RAGAS_GPT_Relevancy'] = gpt_df['answer_relevancy'].values\n",
    "    \n",
    "    # For OSS answers\n",
    "    if 'oss_answer' in df.columns and 'question' in df.columns:\n",
    "        oss_dataset = Dataset.from_dict({\n",
    "            'question': df['question'].tolist(),\n",
    "            'answer': df['oss_answer'].tolist(),\n",
    "            'ground_truth': df['human_answer'].tolist(),\n",
    "            'contexts': [[str(ans)] for ans in df['oss_answer'].tolist()]\n",
    "        })\n",
    "        \n",
    "        print(\"\\nEvaluating OSS answers with RAGAS...\")\n",
    "        oss_results = evaluate(\n",
    "            oss_dataset,\n",
    "            metrics=[answer_correctness, answer_similarity, answer_relevancy],\n",
    "            llm=llm,\n",
    "            embeddings=embeddings\n",
    "        )\n",
    "        \n",
    "        oss_df = oss_results.to_pandas()\n",
    "        df['RAGAS_OSS_Correctness'] = oss_df['answer_correctness'].values\n",
    "        df['RAGAS_OSS_Similarity'] = oss_df['answer_similarity'].values\n",
    "        df['RAGAS_OSS_Relevancy'] = oss_df['answer_relevancy'].values\n",
    "    \n",
    "    # For RAG answers\n",
    "    if 'rag_answer' in df.columns and 'question' in df.columns:\n",
    "        rag_dataset = Dataset.from_dict({\n",
    "            'question': df['question'].tolist(),\n",
    "            'answer': df['rag_answer'].tolist(),\n",
    "            'ground_truth': df['human_answer'].tolist(),\n",
    "            'contexts': [[str(ans)] for ans in df['rag_answer'].tolist()]\n",
    "        })\n",
    "        \n",
    "        print(\"\\nEvaluating RAG answers with RAGAS...\")\n",
    "        rag_results = evaluate(\n",
    "            rag_dataset,\n",
    "            metrics=[answer_correctness, answer_similarity, answer_relevancy, faithfulness],\n",
    "            llm=llm,\n",
    "            embeddings=embeddings\n",
    "        )\n",
    "        \n",
    "        rag_df = rag_results.to_pandas()\n",
    "        df['RAGAS_RAG_Correctness'] = rag_df['answer_correctness'].values\n",
    "        df['RAGAS_RAG_Similarity'] = rag_df['answer_similarity'].values\n",
    "        df['RAGAS_RAG_Relevancy'] = rag_df['answer_relevancy'].values\n",
    "        df['RAGAS_RAG_Faithfulness'] = rag_df['faithfulness'].values\n",
    "    \n",
    "    # Calculate average scores\n",
    "    print(\"\\nCalculating summary statistics...\")\n",
    "    summary = {\n",
    "        'Metric': [],\n",
    "        'GPT_Avg': [],\n",
    "        'OSS_Avg': [],\n",
    "        'RAG_Avg': []\n",
    "    }\n",
    "    \n",
    "    metrics = ['BLEU', 'RAGAS_Correctness', 'RAGAS_Similarity', 'RAGAS_Relevancy']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        summary['Metric'].append(metric)\n",
    "        \n",
    "        gpt_col = f\"{metric}_GPT_vs_Human\" if metric == 'BLEU' else f\"{metric.replace('RAGAS_', 'RAGAS_GPT_')}\"\n",
    "        oss_col = f\"{metric}_OSS_vs_Human\" if metric == 'BLEU' else f\"{metric.replace('RAGAS_', 'RAGAS_OSS_')}\"\n",
    "        rag_col = f\"{metric}_RAG_vs_Human\" if metric == 'BLEU' else f\"{metric.replace('RAGAS_', 'RAGAS_RAG_')}\"\n",
    "        \n",
    "        summary['GPT_Avg'].append(round(df[gpt_col].mean(), 4) if gpt_col in df.columns else 'N/A')\n",
    "        summary['OSS_Avg'].append(round(df[oss_col].mean(), 4) if oss_col in df.columns else 'N/A')\n",
    "        summary['RAG_Avg'].append(round(df[rag_col].mean(), 4) if rag_col in df.columns else 'N/A')\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    \n",
    "    # Save results\n",
    "    output_file = excel_file.replace('.xlsx', '_evaluated.xlsx')\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    print(f\"\\nâœ… Results saved to: {output_file}\")\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    return df, summary_df\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Update with your file path and column names\n",
    "    excel_file = \"your_file.xlsx\"\n",
    "    \n",
    "    # Make sure your Excel has these columns:\n",
    "    # - question\n",
    "    # - human_answer\n",
    "    # - gpt_answer\n",
    "    # - oss_answer\n",
    "    # - rag_answer\n",
    "    \n",
    "    results_df, summary = evaluate_answers(excel_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bda985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0c586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f646737a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2d217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d49c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
